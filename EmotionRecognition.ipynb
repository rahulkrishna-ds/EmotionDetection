{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmotionRecognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIQCbDXEcuq5cw7aviZoAq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulvudari/EmotionDetection/blob/master/EmotionRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjy6HLsRmHW4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOuPyA-BYQyR",
        "outputId": "03cbc8cd-b1aa-48df-a2a5-1b8f909cff39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "#import utils\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "#from tensorflow.keras.utils import plot_model\n",
        "\n",
        "from IPython.display import SVG, Image\n",
        "#from livelossplot import PlotLossesTensorFlowKeras()\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version:\", tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version: 2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asAjFYOWpZkR",
        "outputId": "e8368de5-25c6-4e00-8d28-9cb69ad8e141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.8)\n",
            "Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (0.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.6.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoHcFX8fpuj3"
      },
      "source": [
        "import json\n",
        "import os, zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJwWrNZOpgVk"
      },
      "source": [
        "!mkdir .kaggle\n",
        "token = {\"username\":1'username',\"key\":\"key\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9hhy696M7uy",
        "outputId": "6c5a5fde-a0e0-4d90-9425-36657d05c6ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "\n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "!kaggle config set -n path -v{/content}\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle -v\n",
        "!kaggle datasets download -d jonathanoheix/face-expression-recognition-dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "- path is now set to: {/content}\n",
            "Kaggle API 1.5.4\n",
            "Downloading face-expression-recognition-dataset.zip to {/content}/datasets/jonathanoheix/face-expression-recognition-dataset\n",
            "100% 120M/121M [00:03<00:00, 29.8MB/s]\n",
            "100% 121M/121M [00:03<00:00, 38.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qsb0ATayV-d"
      },
      "source": [
        "path = '/content/{/content}/datasets/jonathanoheix/face-expression-recognition-dataset/face-expression-recognition-dataset.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOY7lPDYy4G2"
      },
      "source": [
        "local_zip = '/content/{/content}/datasets/jonathanoheix/face-expression-recognition-dataset/face-expression-recognition-dataset.zip'\n",
        "\n",
        "zipref = zipfile.ZipFile(local_zip,'r')\n",
        "zipref.extractall('/tmp')\n",
        "zipref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqv_Y-Pw_SQg"
      },
      "source": [
        "base_dir = '/tmp/images/images'\n",
        "train_dir = os.path.join(base_dir,'train')\n",
        "test_dir = os.path.join(base_dir,'validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2MtWgK8_dso",
        "outputId": "3c5d0f0c-f8c0-4ef3-d154-e68d5b50aa83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "img_size = 48\n",
        "batch_size = 64\n",
        "datagen_train = ImageDataGenerator(horizontal_flip=True)\n",
        "train_generator = datagen_train.flow_from_directory(train_dir,\n",
        "                                                    target_size = (img_size, img_size),\n",
        "                                                    color_mode='grayscale',\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    shuffle=True)\n",
        "datagen_validation = ImageDataGenerator(horizontal_flip=True)\n",
        "validation_generator = datagen_validation.flow_from_directory(test_dir,\n",
        "                                                    target_size = (img_size, img_size),\n",
        "                                                    color_mode='grayscale',\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 28821 images belonging to 7 classes.\n",
            "Found 7066 images belonging to 7 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV8OJrDHz1G4",
        "outputId": "b227c631-2abc-4555-b325-18415bb3b718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "#1 - conv\n",
        "\n",
        "model.add(Conv2D(64,(3,3),padding='same',input_shape=(48,48,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "#2 - conv\n",
        "model.add(Conv2D(128,(5,5),padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "#3 - conv\n",
        "model.add(Conv2D(512,(3,3),padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "#4 - conv\n",
        "model.add(Conv2D(512,(3,3),padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "#flatten\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(512))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(7,activation='softmax'))\n",
        "\n",
        "opt = Adam(lr=0.0005)\n",
        "\n",
        "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 48, 48, 64)        640       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 48, 48, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 48, 48, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 24, 24, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 24, 24, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 12, 12, 512)       590336    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 12, 12, 512)       2048      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 6, 6, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               1179904   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 7)                 3591      \n",
            "=================================================================\n",
            "Total params: 4,478,727\n",
            "Trainable params: 4,474,759\n",
            "Non-trainable params: 3,968\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2ODDyo30YLa",
        "outputId": "f2e5c78f-eebb-4956-a31e-40b86ad83e2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 15\n",
        "steps_per_epoch = train_generator.n//train_generator.batch_size\n",
        "validation_steps = validation_generator.n//validation_generator.batch_size\n",
        "\n",
        "checkpoint = ModelCheckpoint('model_weights.h5',monitor='val_accuracy',\n",
        "                            save_weights_only=True,mode='max',verbose=1)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=2,min_lr=0.00001,mode='auto')\n",
        "callbacks = [checkpoint, reduce_lr]\n",
        "\n",
        "history = model.fit(x=train_generator,\n",
        "                   steps_per_epoch=steps_per_epoch,\n",
        "                   epochs=epochs,\n",
        "                   validation_data=validation_generator,\n",
        "                   validation_steps=validation_steps,\n",
        "                   callbacks = callbacks )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "450/450 [==============================] - ETA: 0s - loss: 1.8014 - accuracy: 0.3091\n",
            "Epoch 00001: saving model to model_weights.h5\n",
            "450/450 [==============================] - 1169s 3s/step - loss: 1.8014 - accuracy: 0.3091 - val_loss: 1.8905 - val_accuracy: 0.3405\n",
            "Epoch 2/15\n",
            "450/450 [==============================] - ETA: 0s - loss: 1.4772 - accuracy: 0.4306\n",
            "Epoch 00002: saving model to model_weights.h5\n",
            "450/450 [==============================] - 1179s 3s/step - loss: 1.4772 - accuracy: 0.4306 - val_loss: 1.3676 - val_accuracy: 0.4697\n",
            "Epoch 3/15\n",
            "450/450 [==============================] - ETA: 0s - loss: 1.3371 - accuracy: 0.4867\n",
            "Epoch 00003: saving model to model_weights.h5\n",
            "450/450 [==============================] - 1184s 3s/step - loss: 1.3371 - accuracy: 0.4867 - val_loss: 1.2660 - val_accuracy: 0.5148\n",
            "Epoch 4/15\n",
            "450/450 [==============================] - ETA: 0s - loss: 1.2489 - accuracy: 0.5215\n",
            "Epoch 00004: saving model to model_weights.h5\n",
            "450/450 [==============================] - 1181s 3s/step - loss: 1.2489 - accuracy: 0.5215 - val_loss: 1.1579 - val_accuracy: 0.5601\n",
            "Epoch 5/15\n",
            "450/450 [==============================] - ETA: 0s - loss: 1.1930 - accuracy: 0.5439\n",
            "Epoch 00005: saving model to model_weights.h5\n",
            "450/450 [==============================] - 1156s 3s/step - loss: 1.1930 - accuracy: 0.5439 - val_loss: 1.2109 - val_accuracy: 0.5315\n",
            "Epoch 6/15\n",
            "450/450 [==============================] - ETA: 0s - loss: 1.1566 - accuracy: 0.5634\n",
            "Epoch 00006: saving model to model_weights.h5\n",
            "450/450 [==============================] - 1171s 3s/step - loss: 1.1566 - accuracy: 0.5634 - val_loss: 1.1615 - val_accuracy: 0.5609\n",
            "Epoch 7/15\n",
            "450/450 [==============================] - ETA: 0s - loss: 1.0730 - accuracy: 0.5940\n",
            "Epoch 00007: saving model to model_weights.h5\n",
            "450/450 [==============================] - 1190s 3s/step - loss: 1.0730 - accuracy: 0.5940 - val_loss: 1.0469 - val_accuracy: 0.6080\n",
            "Epoch 8/15\n",
            "450/450 [==============================] - ETA: 0s - loss: 1.0544 - accuracy: 0.5999\n",
            "Epoch 00008: saving model to model_weights.h5\n",
            "450/450 [==============================] - 1192s 3s/step - loss: 1.0544 - accuracy: 0.5999 - val_loss: 1.0403 - val_accuracy: 0.6092\n",
            "Epoch 9/15\n",
            "450/450 [==============================] - ETA: 0s - loss: 1.0428 - accuracy: 0.6085\n",
            "Epoch 00009: saving model to model_weights.h5\n",
            "450/450 [==============================] - 1189s 3s/step - loss: 1.0428 - accuracy: 0.6085 - val_loss: 1.0232 - val_accuracy: 0.6179\n",
            "Epoch 10/15\n",
            "450/450 [==============================] - ETA: 0s - loss: 1.0361 - accuracy: 0.6080\n",
            "Epoch 00010: saving model to model_weights.h5\n",
            "450/450 [==============================] - 1205s 3s/step - loss: 1.0361 - accuracy: 0.6080 - val_loss: 1.0264 - val_accuracy: 0.6145\n",
            "Epoch 11/15\n",
            "450/450 [==============================] - ETA: 0s - loss: 1.0207 - accuracy: 0.6163\n",
            "Epoch 00011: saving model to model_weights.h5\n",
            "450/450 [==============================] - 1185s 3s/step - loss: 1.0207 - accuracy: 0.6163 - val_loss: 1.0220 - val_accuracy: 0.6136\n",
            "Epoch 12/15\n",
            "450/450 [==============================] - ETA: 0s - loss: 1.0113 - accuracy: 0.6153\n",
            "Epoch 00012: saving model to model_weights.h5\n",
            "450/450 [==============================] - 1200s 3s/step - loss: 1.0113 - accuracy: 0.6153 - val_loss: 1.0101 - val_accuracy: 0.6206\n",
            "Epoch 13/15\n",
            "450/450 [==============================] - ETA: 0s - loss: 1.0003 - accuracy: 0.6241\n",
            "Epoch 00013: saving model to model_weights.h5\n",
            "450/450 [==============================] - 1202s 3s/step - loss: 1.0003 - accuracy: 0.6241 - val_loss: 1.0133 - val_accuracy: 0.6200\n",
            "Epoch 14/15\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.9980 - accuracy: 0.6204\n",
            "Epoch 00014: saving model to model_weights.h5\n",
            "450/450 [==============================] - 1202s 3s/step - loss: 0.9980 - accuracy: 0.6204 - val_loss: 1.0113 - val_accuracy: 0.6185\n",
            "Epoch 15/15\n",
            "450/450 [==============================] - ETA: 0s - loss: 0.9795 - accuracy: 0.6285\n",
            "Epoch 00015: saving model to model_weights.h5\n",
            "450/450 [==============================] - 1204s 3s/step - loss: 0.9795 - accuracy: 0.6285 - val_loss: 1.0012 - val_accuracy: 0.6266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeVzGlkmwhOW"
      },
      "source": [
        "model_json = model.to_json()\n",
        "with open(\"model.json\",\"w\") as json_file:\n",
        "    json_file.write(model_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LnmODBd9W0-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7HCCeb3m_NP",
        "outputId": "b098a94f-2bfb-4fc2-ed67-c4db010cd6ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'{'\t\t\t\t       model.json\t  sample_data\n",
            " camera.py\t\t\t       model.py\t\t  templates\n",
            " haarcascade_frontalface_default.xml   model_weights.h5   videoplayback.mp4\n",
            " main.py\t\t\t       __pycache__\n",
            "'{'\t\t\t\t       model.json\t  sample_data\n",
            " camera.py\t\t\t       model.py\t\t  templates\n",
            " haarcascade_frontalface_default.xml   model_weights.h5   videoplayback.mp4\n",
            " main.py\t\t\t       __pycache__\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_P3c7jqzxG4",
        "outputId": "4438e657-77d6-44e7-8c9e-2c3084bd7a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.2)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI5rX1RrlHN5",
        "outputId": "5b042d1b-344c-4126-9473-952d3a0bfc01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        }
      },
      "source": [
        "!python main.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-30 08:30:12.155643: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-30 08:30:13.704790: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2020-09-30 08:30:13.710840: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000144999 Hz\n",
            "2020-09-30 08:30:13.711083: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2e3aa00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-09-30 08:30:13.711119: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-09-30 08:30:13.713137: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-09-30 08:30:13.722488: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2020-09-30 08:30:13.722535: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fa105d5ed84e): /proc/driver/nvidia/version does not exist\n",
            " * Serving Flask app \"main\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n",
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
            " * Running on http://6bf652cfdf94.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n",
            "127.0.0.1 - - [30/Sep/2020 08:30:20] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "[ WARN:0] global /io/opencv/modules/videoio/src/cap_v4l.cpp (802) open VIDEOIO ERROR: V4L: can't open camera by index 0\n",
            "127.0.0.1 - - [30/Sep/2020 08:30:20] \"\u001b[35m\u001b[1mGET /video_feed HTTP/1.1\u001b[0m\" 500 -\n",
            "Error on request:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/werkzeug/serving.py\", line 323, in run_wsgi\n",
            "    execute(self.server.app)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/werkzeug/serving.py\", line 314, in execute\n",
            "    for data in application_iter:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/werkzeug/wsgi.py\", line 506, in __next__\n",
            "    return self._next()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/werkzeug/wrappers/base_response.py\", line 45, in _iter_encoded\n",
            "    for item in iterable:\n",
            "  File \"/content/main.py\", line 16, in gen\n",
            "    frame = camera.get_frame()\n",
            "  File \"/content/camera.py\", line 20, in get_frame\n",
            "    gray_fr = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY)\n",
            "cv2.error: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
            "127.0.0.1 - - [30/Sep/2020 08:30:21] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "Error in atexit._run_exitfuncs:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 298, in _exit_function\n",
            "    if current_process() is not None:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 39, in current_process\n",
            "    return _current_process\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}